---
title: 'Practica 3: Inferencia Bayesiana en el modelo Normal'
output:
  html_document:
---

## Practica

Data:
- 0 is male
- 1 is female

```{r}
library(LearnBayes)

data("birthweight")
summary(birthweight)
```

### Disribucion de los pesos
```{r}
weight_males <- birthweight[birthweight$gender==0,]$weight
weight_females <- birthweight[birthweight$gender==1,]$weight

hist(weight_females)
hist(weight_males)
```

Contrastes: Shapiro test
```{r}
shapiro.test(weight_females)
shapiro.test(weight_males)
```
Podemos acceptar la hipotesis de Normalidad en los grupos

### Inferencia Bayesiana en el modelo normal

#### Intervalos de credibilidad
```{r}
apriori_Jeffrey <- function(mu, sigma){
  1/sigma
}
```

Para niños
```{r}
library(HDInterval)
library(invgamma)

intervalo_male_z <-
  hdi(qt(c(0.025,0.975), df = length(weight_males)))

intervalo_male <- sqrt(var(weight_males)/length(weight_males))*intervalo_male_z + mean(weight_males)
intervalo_male
```

Para niñas
```{r}
intervalo_female_z <-
  hdi(qt(c(0.025,0.975), df = length(weight_females)))

intervalo_female <- sqrt(var(weight_females)/length(weight_females))*intervalo_female_z + mean(weight_females)
intervalo_female
```

#### Contrastes de hipotesis
$$

H_0: \mu_1 = \mu2 \\
H_1: \mu_1 \neq \mu2


$$
Igualdad de varianza (enfoque clasico)
```{r}
library(onewaytests)

var.test(birthweight$weight ~ birthweight$gender)
```
Test de igualidad de media (enfoque clasico)
```{r}
t.test(data = birthweight, weight ~ gender, var.equal = T)
```

Con el enfoque Bayesiano: evidencia debil para H0 
(coerencia con el p-valor obtenido con el enfoque clasico)
```{r}
library(BayesFactor)

ttestBF(weight_males, weight_females)
```

#### Estimador de Bayesiano con apriori discreta
Apriori
```{r}
theta <- 2751:3350
n_males <- length(weight_males)

priors <- rep(1/length(theta), length(theta))
```

Verosimilitud
```{r}
sigma <- sd(weight_males)

likelihood <- function(theta){
  L <- 1
  
  for(i in 1:n_males){
    L <- L*dnorm(weight_males[i], mean = theta, sd = sigma)
  }
  return(L)
}
```

Aposteriori
```{r}
posteriors <- likelihood(theta)*priors
posteriors <- posteriors/sum(posteriors)

plot(posteriors)
```
Estimadores de Bayes
- media
```{r}
Bayes_est_mean <- sum(posteriors*theta)
Bayes_est_mean
```
- mediana 
```{r}
library(spatstat)

weighted.median(theta, posteriors)
```

Intervalo de credibilidad
```{r}
library(magrittr)
library(dplyr)

z <- cbind(theta, posteriors) %>%
  as.data.frame() %>%
  mutate(z = cumsum(posteriors) > 0.95)

head(z)
```

#### Contraste de independencia
```{r}
premature <- ifelse(birthweight$age > 38, "prematuro", "normal")

table_premature <- table(premature, birthweight$gender)
table_premature

contingencyTableBF(
  table_premature, 
  sampleType = "jointMulti", 
  priorConcentration = 1
)
```
## Ejemplos

### 1) Eleccion de una apriori
```{r}
mu <- 100
tau <- 12.16
sigma <- 15
n <- 4
se <- sigma/sqrt(4)
ybar <- c(110,125,140)
tau1 <- 1/sqrt(1/se^2+1/tau^2)
mu1 <- (ybar/se^2+mu/tau^2)*tau1^2

summ1 <- cbind(ybar,mu1,tau1)
summ1
```


```{r}
#Caso 1
curve(dnorm(x,mean=mu,sd=tau),xlim=c(mu-3*tau,mu+3*tau),ylim=c(0,0.08),col="red",
      ylab="Comparativa de densidades")
curve(dnorm(x,mean=ybar[1],sd=se),xlim=c(mu-3*tau,mu+3*tau),ylim=c(0,0.05),col="blue",add=T)
curve(dnorm(x,mean=mu1[1],sd=tau1),xlim=c(mu-3*tau,mu+3*tau),ylim=c(0,0.05),col="green",add=T)

#Caso 2
curve(dnorm(x,mean=mu,sd=tau),xlim=c(mu-3*tau,150),ylim=c(0,0.08),col="red",
      ylab="Comparativa de densidades")
curve(dnorm(x,mean=ybar[2],sd=se),xlim=c(mu-3*tau,150),ylim=c(0,0.05),col="blue",add=T)
curve(dnorm(x,mean=mu1[2],sd=tau1),xlim=c(mu-3*tau,150),ylim=c(0,0.05),col="green",add=T)

#Caso 3
curve(dnorm(x,mean=mu,sd=tau),xlim=c(mu-3*tau,180),ylim=c(0,0.08),col="red",
      ylab="Comparativa de densidades")
curve(dnorm(x,mean=ybar[3],sd=se),xlim=c(mu-3*tau,180),ylim=c(0,0.05),col="blue",add=T)
curve(dnorm(x,mean=mu1[3],sd=tau1),xlim=c(mu-3*tau,150),ylim=c(0,0.05),col="green",add=T)
```

```{r}
# Estudiemos ahora la influencia de elegir una t de student como apriori
tscale=20/qt(0.95,2)
tscale


#Representemos ahora en un mismo gráfico las dos posibles a priori
curve(dnorm(x,mean=mu,sd=tau),xlim=c(mu-3*tau,mu+3*tau),ylim=c(0,0.06),col="red",
      ylab="Dos posibles a priori")
curve(1/tscale*dt((x-mu)/tscale,df=2),xlim=c(mu-3*tau,mu+3*tau),ylim=c(0,0.08),col="blue",add=T)

#Ahora vamos a hacer los cálculos de la a posteriori usando una a priori $t$-Student. 
summ2=c()
for (i in 1:3){
  theta=seq(60,180,length=500)
  like= dnorm((theta-ybar[i])/7.5)
  prior=dt((theta-mu)/tscale,2)
  post=like*prior
  post=post/sum(post)
  m= sum(theta*post)
  s=sqrt(sum(theta^2*post)-m^2)
  summ2=rbind(summ2,c(ybar[i],m,s))
}
summ2


#Ahora vamos a comparar las dos a posterioris, normal frente a $t$-student.

cbind(summ1,summ2)
```

### 2) Varianza conocida
```{r}
library(LearnBayes)

data("marathontimes")
time <- marathontimes$time


hist(marathontimes$time, 
     ylab="Frecuencia absoluta", xlab="Tiempos",
     main="Histograma de los tiempos de la marathon")

boxplot(marathontimes$time,
        main="Diagrama de Cajas")

shapiro.test(time)
qqnorm(time)
```
```{r}
library(ggplot2)
library(qqplotr)
datos=data.frame(marathontimes)

gg <- ggplot(data = datos, mapping = aes(sample = time)) +
  geom_qq_band(bandType = "boot",  alpha = 0.25) +
  stat_qq_line() +
  stat_qq_point() +
  labs(x = "Cuantiles Teóricos", y = "Cuantiles Muestrales")
gg
```

```{r}
#A continuación seleccionamos una normal a priori con la información de los percentiles que se dispone.
n <- length(time)
media <- mean(time)
q1 <- list(p=0.1,x=190)
q2 <- list(p=0.9,x=350)

nn <- normal.select(q1,q2)

mu0 <- nn$mu
s0 <- nn$sigma
```

```{r}
#Supongamos que la desviación típica de la verosimilitud es conocida y vale 
s <- 75
muposte <- (n*s0^2*mean(time)+s^2*mu0)/(n*s0^2+s^2)

sposte <- sqrt((s0^2*s^2)/(n*s0^2+s^2))

mm <- cbind(muposte,sposte)
print(mm)


curve(dnorm(x,mean=mu0,sd=s0),xlim=c(mu0-3*s0,mu0+3*s0),ylim=c(0,0.05),col="red",
      ylab="Comparativa de densidades")

curve(dnorm(x,mean=muposte,sd=sposte),xlim=c(muposte-4*sposte,muposte+4*sposte),
      ylim=c(0,0.05),add=TRUE,col="blue")
```

Intervalo de credibilidad
```{r}
#Cálculo de la Región de Credibilidad

inferior <- muposte-qnorm(0.975,0,1)*sposte
superior <- muposte+qnorm(0.975,0,1)*sposte
rc <- cbind(inferior,superior)
print(rc)
```

Contraste de Bayes
```{r}
#Factor Bayes para  H_0: mu <= 310  frente a  H_1: mu > 310
f0=pnorm(310,mu0,s0)
f1=1-f0

f0/f1

f0poste=pnorm(310,muposte,sposte)
f0poste
f1poste=1-f0poste

f0poste/f1poste

BF01=(f0poste*f1)/(f1poste*f0)

print(BF01)

#Por tanto, según los valores de la tabla de Jeffrey, hay una evidencia fuerte hacia que la hipótesis mu <= 310.
```

### 3) Distribucion Normal Gamma Invertida
```{r}
library("plot3Drgl")
library(plot3D)
require(rgl)

# Parametros:

m <- 10 # valor de la media
c <- 1/10 # valor de la constante
a <- 1# primer parámetro de la gamma invertida
b <- 4 # segundo valor de la gamma invertida


x <- seq(m-2, m+2, length.out = 50)  
y <- seq(0.11, 5, length.out = 25) 

ngai <- function(x,y) {
  b^a/(gamma(a)) *(1/y^(a+1))*exp(-b/y)*(1/(sqrt(2*pi)))*exp(-(x-m)^2/(2*y*c))
}

z <- outer(x, y, ngai)  
persp(x, y, z,main="Perspectiva Cónica",
      zlab = "Height",
      theta = 30, phi = 15,
      col = "springgreen", shade = 0.5)

persp3D(x,y,z,theta=30, phi=50, axes=TRUE,scale=2, box=TRUE, nticks=5, 
        
        ticktype="detailed", xlab="X-value", ylab="Y-value", zlab="Z-value", 
        
        main="Normal Gamma Invertida")
```

### 4) Varianza desconocida
```{r}
library(LearnBayes)
data("marathontimes")

time <- marathontimes$time

hist(time,freq=F)
curve(dnorm(x,mean=mean(time),sd=sd(time)),add=T,col="blue")

shapiro.test(time)
t.test(time)
```
```{r}
hist(time,freq=F)
curve(dnorm(x,mean=255,sd=sd(time)),add=T,col="red")
curve(dnorm(x,mean=265,sd=sd(time)),add=T,col="green")
curve(dnorm(x,mean=285,sd=sd(time)),add=T,col="purple")
curve(dnorm(x,mean=300,sd=sd(time)),add=T,col="blue")
curve(dnorm(x,mean=400,sd=sd(time)),add=T,col="blue")
```

```{r}
#Hagamos lo mismo para la varianza
n=length(time)
uppert=(n-1)*sd(time)^2/qchisq(0.025,df=n-1);sqrt(uppert)
lowert=(n-1)*sd(time)^2/qchisq(0.975,df=n-1);sqrt(lowert)

#Representemos curvas normales para diferentes desviaciones típicas.
hist(time,freq=F)
curve(dnorm(x,mean=255,sd=39),add=T,col="red")
curve(dnorm(x,mean=265,sd=45),add=T,col="green")
curve(dnorm(x,mean=285,sd=37.5),add=T,col="purple")
curve(dnorm(x,mean=300,sd=70),add=T,col="blue")
curve(dnorm(x,mean=270,sd=100),add=T,col="blue")
```

```{r}
#A continuación representamos el contorno de la normal-gamma-invertida

d=mycontour(normchi2post,c(220,330,500,10000),time)
title(xlab="Media",ylab="Varianza")

Su=sum((time-mean(time))^2)
n=length(time)
S=Su/(n) # Se usará más adelante
sigma2=(Su)/rchisq(1000,df=n)
mu=rnorm(1000,mean=mean(time),sd=sqrt(sigma2)/sqrt(n))
```
```{r}
#A continuación se muestran todos estos datos simulados en un contorno.


d=mycontour(normchi2post,c(220,330,500,10000),time)
title(xlab="Media",ylab="Varianza")
points(mu,sigma2)

```

```{r}
# A continuación se calculan los HPDI para las marginales correspondientes

library(HDInterval)
dens2 <- density(sigma2)
sqrt(hdi(dens2, credMass=0.90))

tsup=qt(0.975,n)
ss=mean(time)+tsup*sqrt(S/n)
ii=mean(time)-tsup*sqrt(S/n)
print(c(ii,ss))
```

